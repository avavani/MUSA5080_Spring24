knitr::opts_chunk$set(echo = TRUE)
library(GGally)
p_load(GGally,MASS,ISLR2)
library(pacman)
p_load(GGally,MASS,ISLR2)
# Produce scatterplot matrix
ggpairs(Auto[1:8])
# Compute the matrix of correlations (excluding 'name' variable)
correlation_matrix <- cor(Auto[, -9])  # Excluding the 9th column (name)
# Print correlation matrix
print(correlation_matrix)
# Print correlation matrix
print(correlation_matrix)
lm.fit <- lm(mpg ~ ., data = Auto)
summary(lm.fit)
lm.fit1 <- update(lm.fit , ~ . - name)
summary(lm.fit1)
lm.fit <- lm(log(mpg) ~ ., data = Auto)
summary(lm.fit)
lm.fit1 <- update(lm.fit , ~ . - name)
summary(lm.fit1)
#diagnostic plots
par(mfrow = c(2, 2))
plot(lm.fit1)
#diagnostic plots
par(mfrow = c(2, 2))
plot(lm.fit1)
# Calculate Variance Inflation Factors (VIF)
library(car)
vif(lm.fit1)
model2 <- lm(mpg ~ displacement + weight +acceleration + year + origin, data = Auto)
summary(model2)
par(mfrow = c(2, 2))
plot(model2)
vif(model2)
model3 <- lm(mpg ~ I(weight^2) +acceleration + year + origin, data = Auto)
summary(model3)
par(mfrow = c(2, 2))
plot(model3)
vif(model3)
# Plot standardized residuals vs. leverage
plot(model2, which = 5)
# Identify influential points using Cook's distance
influenceIndexPlot(model2, id.method = "identify", main = "Cook's Distance Plot")
plot(model2, which = 4)
set.seed(1)
train <- sample(392, 196)
lm.fit <- lm(mpg ~ horsepower, data = Auto, subset = train)
attach(Auto)
mean((mpg - predict(lm.fit, Auto))[-train]^2)
lm.fit2 <- lm(mpg ~ poly(horsepower, 2), data = Auto,
subset = train)
mean((mpg - predict(lm.fit2, Auto))[-train]^2)
lm.fit3 <- lm(mpg ~ poly(horsepower, 3), data = Auto,
subset = train)
mean((mpg - predict(lm.fit3, Auto))[-train]^2)
set.seed(2)
train <- sample(392, 196)
lm.fit <- lm(mpg ~ horsepower, subset = train)
mean((mpg - predict(lm.fit, Auto))[-train]^2)
lm.fit2 <- lm(mpg ~ poly(horsepower, 2), data = Auto,
subset = train)
mean((mpg - predict(lm.fit2, Auto))[-train]^2)
lm.fit3 <- lm(mpg ~ poly(horsepower, 3), data = Auto,
subset = train)
mean((mpg - predict(lm.fit3, Auto))[-train]^2)
glm.fit <- glm(mpg ~ horsepower, data = Auto)
coef(glm.fit)
lm.fit <- lm(mpg ~ horsepower, data = Auto)
coef(lm.fit)
library(boot)
glm.fit <- glm(mpg ~ horsepower, data = Auto)
cv.err <- cv.glm(Auto, glm.fit)
cv.err$delta
cv.error <- rep(0, 10)
for (i in 1:10) {
glm.fit <- glm(mpg ~ poly(horsepower, i), data = Auto)
cv.error[i] <- cv.glm(Auto, glm.fit)$delta[1]
}
cv.error
set.seed(17)
cv.error.10 <- rep(0, 10)
for (i in 1:10) {
glm.fit <- glm(mpg ~ poly(horsepower, i), data = Auto)
cv.error.10[i] <- cv.glm(Auto, glm.fit, K = 10)$delta[1]
}
cv.error.10
lm.fit <- lm(log(mpg) ~ ., data = Auto)
summary(lm.fit)
lm.fit1 <- update(lm.fit , ~ . - name)
summary(lm.fit1)
#diagnostic plots
par(mfrow = c(2, 2))
plot(lm.fit1)
set.seed(1)
train <- sample(392, 196)
lm.fit <- lm(mpg ~ horsepower, data = Auto, subset = train)
attach(Auto)
mean((mpg - predict(lm.fit, Auto))[-train]^2)
lm.fit2 <- lm(mpg ~ poly(horsepower, 2), data = Auto,
subset = train)
mean((mpg - predict(lm.fit2, Auto))[-train]^2)
lm.fit3 <- lm(mpg ~ poly(horsepower, 3), data = Auto,
subset = train)
mean((mpg - predict(lm.fit3, Auto))[-train]^2)
set.seed(2)
train <- sample(392, 196)
lm.fit <- lm(mpg ~ horsepower, subset = train)
mean((mpg - predict(lm.fit, Auto))[-train]^2)
lm.fit2 <- lm(mpg ~ poly(horsepower, 2), data = Auto,
subset = train)
mean((mpg - predict(lm.fit2, Auto))[-train]^2)
lm.fit3 <- lm(mpg ~ poly(horsepower, 3), data = Auto,
subset = train)
mean((mpg - predict(lm.fit3, Auto))[-train]^2)
glm.fit <- glm(mpg ~ horsepower, data = Auto)
coef(glm.fit)
lm.fit <- lm(mpg ~ horsepower, data = Auto)
coef(lm.fit)
# Load some libraries
library(pacman)
library(pacman)
p_load(tidyverse,sf,spdep,caret,ckanr,FNN,grid,gridExtra,ggcorrplot) # plot correlation plot
p_load(corrr,kableExtra,jtools,ggstance,ggpubr,broom.mixed,RColorBrewer)
# functions and data directory
root.dir = "https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/DATA/"
source("https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/functions.r")
nhoods <-
st_read("https://github.com/ECDelmelle/MUSA5080_Spring24/blob/main/Week7/BPDA_Neighborhood_Boundaries.geojson") %>%
st_transform('ESRI:102286')
root.dir = "https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/DATA/"
source("https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/functions.r")
nhoods <-
st_read("https://github.com/ECDelmelle/MUSA5080_Spring24/blob/main/Week7/BPDA_Neighborhood_Boundaries.geojson") %>%
st_transform('ESRI:102286')
nhoods <-
st_read("BPDA_Neighborhood_Boundaries.geojson") %>%
st_transform('ESRI:102286')
boston <-
read.csv(file.path(root.dir,"/Chapter3_4/bostonHousePriceData_clean.csv"))
boston.sf <-
boston %>%
st_as_sf(coords = c("Longitude", "Latitude"), crs = 4326, agr = "constant") %>%
st_transform('ESRI:102286')
bostonCrimes <- read.csv(file.path(root.dir,"/Chapter3_4/bostonCrimes.csv"))
# finding counts by group
bostonCrimes %>%
group_by(OFFENSE_CODE_GROUP) %>%
summarize(count = n()) %>%
arrange(-count) %>% top_n(10) %>%
kable() %>%
kable_styling()
# ggplot, reorder
boston.sf$quintiles <- ntile(boston.sf$PricePerSq, 5)
# Mapping data
ggplot() +
geom_sf(data = nhoods, fill = "lightgrey", col = "white") +
geom_sf(data = boston.sf, aes(colour = q5(PricePerSq)),
show.legend = "point", size = .75) +
scale_colour_manual(values = palette5,
labels=qBr(boston,"PricePerSq"),
name="Quintile\nBreaks") +
labs(title="Price Per Square Foot, Boston") +
theme_void()
ggplot() +
geom_sf(data = nhoods, fill = "lightgrey", col = "white") +
geom_sf(data = boston.sf, aes(colour = q5(PricePerSq)),
show.legend = "point", size = .75) +
scale_colour_manual(values = palette,
labels=qBr(boston,"PricePerSq"),
name="Quintile\nBreaks") +
labs(title="Price Per Square Foot, Boston") +
theme_void()
?scale_colour_manual
# ggplot, reorder
boston.sf$quintiles <- ntile(boston.sf$PricePerSq, 5)
palette5 <- brewer.pal(5, "YnGnBu")
palette5 <- brewer.pal(5, "YlGnBu")
# Mapping data
ggplot() +
geom_sf(data = nhoods, fill = "lightgrey", col = "white") +
geom_sf(data = boston.sf, aes(colour = q5(PricePerSq)),
show.legend = "point", size = .75) +
scale_colour_manual(values = palette5,
labels=qBr(boston,"PricePerSq"),
name="Quintile\nBreaks") +
labs(title="Price Per Square Foot, Boston") +
theme_void()
bostonCrimes.sf <-
bostonCrimes %>%
filter(OFFENSE_CODE_GROUP == "Aggravated Assault",
Lat > -1) %>%
dplyr::select(Lat, Long) %>%
na.omit() %>%
st_as_sf(coords = c("Long", "Lat"), crs = "EPSG:4326") %>%
st_transform('ESRI:102286') %>%
distinct()
# Counts of crime per buffer of house sale
boston.sf$crimes.Buffer <- boston.sf %>%
st_buffer(660) %>%
aggregate(mutate(bostonCrimes.sf, counter = 1),., sum) %>%
pull(counter)
## Nearest Neighbor Feature
boston.sf <-
boston.sf %>%
mutate(
crime_nn1 = nn_function(st_coordinates(boston.sf),
st_coordinates(bostonCrimes.sf), k = 1),
crime_nn2 = nn_function(st_coordinates(boston.sf),
st_coordinates(bostonCrimes.sf), k = 2),
crime_nn3 = nn_function(st_coordinates(boston.sf),
st_coordinates(bostonCrimes.sf), k = 3),
crime_nn4 = nn_function(st_coordinates(boston.sf),
st_coordinates(bostonCrimes.sf), k = 4),
crime_nn5 = nn_function(st_coordinates(boston.sf),
st_coordinates(bostonCrimes.sf), k = 5))
# Create the plot with adjusted plot extent
ggplot() +
stat_density_2d(data = data.frame(st_coordinates(bostonCrimes.sf)),
aes(x = X, y = Y, fill = after_stat(level)),
size = 0.1, bins = 20, geom = "polygon") +  # Add 2D density plot
geom_sf(data = nhoods, fill = "transparent", color = "black") +  # Add neighborhood boundaries
scale_fill_gradient(low = "#FFEDA0", high = "#800026", name = "Density", na.value = "grey40", labels = scales::number_format(scale = 1e3)) +  # Color gradient for density
labs(title = "Density of Aggravated Assaults, Boston") +
theme_void()
# Remove geometry column (if present), calculate Age, and select relevant variables
boston_cleaned <- st_drop_geometry(boston.sf) %>%
mutate(Age = 2015 - YR_BUILT) %>%        # Calculate Age based on the year built
dplyr::select(SalePrice, LivingArea, Age, GROSS_AREA) %>%  # Select relevant variables
filter(SalePrice <= 1000000, Age < 500)  # Filter data based on conditions
# Reshape data from wide to long format. This is for mapping purposes below.
boston_long <- gather(boston_cleaned, Variable, Value, -SalePrice)
# Create scatterplot with linear regression lines for each variable
ggplot(boston_long, aes(Value, SalePrice)) +   # Set up the plot
geom_point(size = .5) +                       # Add points for each data point
geom_smooth(method = "lm", se = FALSE, colour = "#FA7800") +  # Add linear regression lines
facet_wrap(~Variable, ncol = 3, scales = "free") +  # Create separate plots for each variable
labs(title = "Price as a function of continuous variables") +  # Set plot title
theme_minimal()  # Apply a minimal theme to the plot
# Clean and prepare the data
boston_cleaned <- boston.sf %>%
st_drop_geometry() %>%                   # Remove geometry column (if present)
mutate(Age = 2015 - YR_BUILT) %>%        # Calculate Age based on the year built
select(SalePrice, starts_with("crime_")) %>%  # Select variables related to crime
filter(SalePrice <= 1000000)             # Filter data based on SalePrice
# Reshape data from wide to long format
boston_long <- gather(boston_cleaned, Variable, Value, -SalePrice)
# Create scatterplot with linear regression lines for each variable
ggplot(boston_long, aes(Value, SalePrice)) +   # Set up the plot
geom_point(size = .5) +                       # Add points for each data point
geom_smooth(method = "lm", se = FALSE, colour = "#FA7800") +  # Add linear regression lines
facet_wrap(~Variable, nrow = 1, scales = "free") +  # Create separate plots for each variable
labs(title = "Price as a function of continuous variables") +  # Set plot title
theme_minimal()  # Apply a minimal theme to the plot
# Select only numeric variables and remove rows with missing values
numericVars <- boston.sf %>%
st_drop_geometry() %>%  # Remove geometry column if present
select_if(is.numeric) %>%  # Select only numeric variables
na.omit()  # Remove rows with missing values
# Calculate correlation matrix
correlation_matrix <- cor(numericVars)
# Create correlation plot
ggcorrplot(
correlation_matrix,  # Correlation matrix
p.mat = cor_pmat(numericVars),  # p-values for significance
colors = c("#25CB10", "white", "#FA7800"),  # Custom color palette
type = "lower",  # Lower triangle of the correlation matrix
insig = "blank"  # Hide insignificant correlations
) +
labs(title = "Correlation across numeric variables")  # Set plot title
# yet another way to plot the correlation plot using the corrr library
numericVars %>%
correlate() %>%
autoplot() +
geom_text(aes(label = round(r,digits=2)),size = 2)
# Load necessary libraries
# Define the control parameters for k-fold cross-validation
control <- trainControl(method = "cv",    # Use k-fold cross-validation
number = 10,      # Number of folds
verboseIter = TRUE,  # Show verbose output
returnData = FALSE,  # Don't return resampled data
savePredictions = TRUE,  # Save predictions
classProbs = FALSE,  # Don't compute class probabilities
summaryFunction = defaultSummary)  # Use default summary function
# Train the linear regression model using k-fold cross-validation
lm_cv <- train(SalePrice ~ LivingArea,  # Formula for the linear regression model
data = boston_sub_200k,  # Dataset
method = "lm",           # Specify "lm" for linear regression
trControl = control)     # Use the defined control parameters
boston_sub_200k <- st_drop_geometry(boston.sf) %>%
filter(SalePrice <= 2000000)
cor.test(boston_sub_200k$LivingArea,
boston_sub_200k$SalePrice,
method = "pearson")
ggscatter(boston_sub_200k,
x = "LivingArea",
y = "SalePrice",
add = "reg.line") +
stat_cor(label.y = 2500000)
# Fit linear regression model
livingReg <- lm(SalePrice ~ LivingArea, data = boston_sub_200k)
# Print summary of regression results
summary(livingReg)
# Create scatterplot with regression line
ggscatter(
data = boston_sub_200k,
x = "LivingArea",            # Predictor variable (x-axis)
y = "SalePrice",             # Outcome variable (y-axis)
add = "reg.line"             # Add regression line to the plot
) +
# Add correlation coefficient and p-value to the plot
stat_cor(
aes(label = paste(..rr.label.., ..p.label.., sep = "~`,`~")),
label.y = 2500000          # Adjust label position on the y-axis
) +
# Add equation of the regression line to the plot
stat_regline_equation(label.y = 2250000)
# View coefficients of the linear regression model
coefficients(livingReg)
# Define new LivingArea value
new_LivingArea <- 4000
# Calculate predicted SalePrice "by hand"
predicted_by_hand <- 378370.01571 + 88.34939 * new_LivingArea
# Predict SalePrice using the predict() function
predicted_with_predict <- predict(livingReg, newdata = data.frame(LivingArea = 4000))
# View the predicted values
predicted_by_hand
predicted_with_predict
reg1 <- lm(SalePrice ~ ., data = boston_sub_200k %>%
dplyr::select(SalePrice, LivingArea, Style,
GROSS_AREA, R_TOTAL_RM, NUM_FLOORS,
R_BDRMS, R_FULL_BTH, R_HALF_BTH,
R_KITCH, R_AC, R_FPLACE))
summary(reg1)
## Plot of marginal response
effect_plot(reg1, pred = R_BDRMS, interval = TRUE, plot.points = TRUE)
## Plot coefficients
plot_summs(reg1, scale = TRUE)
## plot multiple model coeffs
plot_summs(reg1, livingReg)
# Load necessary libraries
# Define the control parameters for k-fold cross-validation
control <- trainControl(method = "cv",    # Use k-fold cross-validation
number = 10,      # Number of folds
verboseIter = TRUE,  # Show verbose output
returnData = FALSE,  # Don't return resampled data
savePredictions = TRUE,  # Save predictions
classProbs = FALSE,  # Don't compute class probabilities
summaryFunction = defaultSummary)  # Use default summary function
# Train the linear regression model using k-fold cross-validation
lm_cv <- train(SalePrice ~ LivingArea,  # Formula for the linear regression model
data = boston_sub_200k,  # Dataset
method = "lm",           # Specify "lm" for linear regression
trControl = control)     # Use the defined control parameters
# View the cross-validation results
print(lm_cv)
# Plot observed versus predicted values
plot(lm_cv$pred$obs, lm_cv$pred$pred,
xlab = "Observed", ylab = "Predicted",
main = "Observed vs Predicted Values")
# Add a diagonal line for reference
abline(0, 1, col = "red")
# Set up leave-one-out cross-validation
control <- trainControl(method = "LOOCV",     # Use leave-one-out cross-validation
number = nrow(boston_sub_200k),  # Number of folds = number of observations
verboseIter = TRUE,  # Show verbose output
returnData = FALSE,  # Don't return resampled data
savePredictions = TRUE,  # Save predictions
classProbs = FALSE,  # Don't compute class probabilities
summaryFunction = defaultSummary)  # Use default summary function
# Train the linear regression model using leave-one-out cross-validation
lm_loocv <- train(SalePrice ~ LivingArea,  # Formula for the linear regression model
data = boston_sub_200k,  # Dataset
method = "lm",           # Specify "lm" for linear regression
trControl = control)     # Use the defined control parameters
# View the cross-validation results
print(lm_loocv)
