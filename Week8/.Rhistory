install.packages(pacman)
library(pacman)
install.packages("pacman")
library(pacman)
install.packages("pacman")
library(pacman)
p_load(tidyverse,sf, spdep, caret, ckanr, FNN, grid, gridExtra, ggcorrplot,
jtools, broom, tufte, rmarkdown, kableExtra, tidycensus)
install.packages("pacman")
library(pacman)
p_load(tidyverse,sf, spdep, caret, ckanr, FNN, grid, gridExtra, ggcorrplot,
jtools, broom, tufte, rmarkdown, kableExtra, tidycensus)
# functions and data directory
root.dir = "https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/DATA/"
source("https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/functions.r")
boston.sf <- st_read(file.path(root.dir,"/Chapter3_4/boston_sf_Ch1_wrangled.geojson")) %>%
st_set_crs('ESRI:102286')
nhoods <-
st_read("https://raw.githubusercontent.com/mafichman/musa_5080_2023/main/Week_4/neighborhoods/bost_nhoods.geojson") %>%
st_transform('ESRI:102286')
# Split the dataset into a training set and a test set using stratified sampling
inTrain <- createDataPartition(
y = paste(boston.sf$Name, boston.sf$NUM_FLOORS.cat,
boston.sf$Style, boston.sf$R_AC),
p = .60, list = FALSE)  # Create a vector of indices for the training set
# Subset the dataset to create the training set
boston.training <- boston.sf[inTrain,]  # Training set
# Subset the dataset to create the test set
boston.test <- boston.sf[-inTrain,]     # Test set
# Fit a linear regression model to predict SalePrice using selected predictors
reg.training <-
lm(SalePrice ~ ., data = as.data.frame(boston.training) %>%
dplyr::select(SalePrice, LivingArea, Style,
GROSS_AREA, NUM_FLOORS.cat,
R_BDRMS, R_FULL_BTH, R_HALF_BTH,
R_KITCH, R_AC, R_FPLACE, crimes.Buffer))
# Make predictions on the test set and evaluate model performance
boston.test <-
boston.test %>%  # Pipe the test set into the following operations
# Add a column indicating the type of regression model used
mutate(Regression = "Baseline Regression",
# Predict sale prices using the trained regression model
SalePrice.Predict = predict(reg.training, boston.test),
# Calculate the difference between predicted and actual sale prices
SalePrice.Error = SalePrice.Predict - SalePrice,
# Calculate the absolute difference between predicted and actual sale prices
SalePrice.AbsError = abs(SalePrice.Predict - SalePrice),
# Calculate the absolute percentage error
SalePrice.APE = (abs(SalePrice.Predict - SalePrice)) / SalePrice) %>%
filter(SalePrice < 5000000)  # Filter out records with SalePrice greater than $5,000,000
mean(boston.test$SalePrice.AbsError, na.rm = T)
mean(boston.test$SalePrice.APE, na.rm = T)
# Set up cross-validation for model evaluation
fitControl <- trainControl(method = "cv", number = 100)
# Set the seed for reproducibility
set.seed(825)
# Train a linear regression model using cross-validation
reg.cv <-
train(SalePrice ~ .,
data = st_drop_geometry(boston.sf) %>%
dplyr::select(SalePrice, LivingArea, Style, GROSS_AREA,
NUM_FLOORS.cat, R_BDRMS, R_FULL_BTH, R_HALF_BTH,
R_KITCH, R_AC, R_FPLACE, crimes.Buffer),
method = "lm",  # Specify the modeling method as linear regression
trControl = fitControl,  # Specify the cross-validation settings
na.action = na.pass)  # Specify how to handle missing values
# View the results of the cross-validated linear regression model
reg.cv
# Display the first 5 rows of the 'resample' data frame within the 'reg.cv' object
reg.cv$resample[1:5,]
# Extract the coordinates from the 'boston.sf' spatial dataframe
coords <- st_coordinates(boston.sf)
# Create a neighbor list using k-nearest neighbors (KNN) with k=5
neighborList <- knn2nb(knearneigh(coords, 5))
# Convert the neighbor list to a spatial weights matrix
spatialWeights <- nb2listw(neighborList, style="W")
# Compute the spatial lag of the variable 'SalePrice' using the spatial weights matrix
boston.sf$lagPrice <- lag.listw(spatialWeights, boston.sf$SalePrice)
# Extract the coordinates from the 'boston.sf' spatial dataframe
coords <- st_coordinates(boston.sf)
# Create a neighbor list using k-nearest neighbors (KNN) with k=5
neighborList <- knn2nb(knearneigh(coords, 5))
# Convert the neighbor list to a spatial weights matrix
spatialWeights <- nb2listw(neighborList, style="W")
# Compute the spatial lag of the variable 'SalePrice' using the spatial weights matrix
boston.sf$lagPrice <- lag.listw(spatialWeights, boston.sf$SalePrice)
# Extract the coordinates of the test dataset
coords.test <- st_coordinates(boston.test)
# Create a neighbor list using k-nearest neighbors (KNN) with k=5 for the test dataset
neighborList.test <- knn2nb(knearneigh(coords.test, 5))
# Convert the neighbor list to a spatial weights matrix for the test dataset
spatialWeights.test <- nb2listw(neighborList.test, style="W")
# Compute the spatial lag of the 'SalePrice.Error' variable for each observation in the test dataset
# by averaging the values of 'SalePrice.Error' for its nearest neighbors using the spatial weights matrix,
# and add the result as a new variable 'lagPriceError' to the test dataset
boston.test %>%
mutate(lagPriceError = lag.listw(spatialWeights.test, SalePrice.Error)) %>%
# Create a scatter plot to visualize the relationship between 'lagPriceError' and 'SalePrice.Error'
ggplot()+
geom_point(aes(x = lagPriceError, y = SalePrice.Error))
moranTest <- moran.mc(boston.test$SalePrice.Error,
spatialWeights.test, nsim = 999)
ggplot(as.data.frame(moranTest$res[c(1:999)]), aes(moranTest$res[c(1:999)])) +
geom_histogram(binwidth = 0.01) +
geom_vline(aes(xintercept = moranTest$statistic), colour = "#FA7800",size=1) +
scale_x_continuous(limits = c(-1, 1)) +
labs(title="Observed and permuted Moran's I",
subtitle= "Observed Moran's I in orange",
x="Moran's I",
y="Count") +
theme_minimal()
moranTest$statistic
# This code chunk performs several operations on the 'boston.test' dataset and formats the result into a visually appealing table using 'kable' and 'kable_styling'.
# Convert the 'boston.test' dataset to a regular data frame
boston.test %>%
as.data.frame() %>%
# Group the data frame by the 'Name' variable
group_by(Name) %>%
# Calculate the mean of 'SalePrice.Predict' and 'SalePrice' variables within each group
summarize(meanPrediction = mean(SalePrice.Predict),
meanPrice = mean(SalePrice)) %>%
# Format the summarized data frame as an HTML table
kable() %>%
# Apply styling to enhance the appearance of the HTML table
kable_styling()
# This code fits a linear regression model ('reg.nhood') to predict 'SalePrice' using various predictors, including neighborhood-related variables, using the training dataset.
# Fit a linear regression model to the training dataset ('boston.training')
reg.nhood <- lm(SalePrice ~ .,
data = as.data.frame(boston.training) %>%
dplyr::select(Name, SalePrice, LivingArea,
Style, GROSS_AREA, NUM_FLOORS.cat,
R_BDRMS, R_FULL_BTH, R_HALF_BTH,
R_KITCH, R_AC, R_FPLACE, crimes.Buffer))
#View the model stats
summary(reg.nhood)
# Create a new dataset ('boston.test.nhood') by predicting 'SalePrice' for the test dataset ('boston.test') using the neighborhood effects model ('reg.nhood')
boston.test.nhood <-
boston.test %>%
mutate(Regression = "Neighborhood Effects",  # Add a new variable indicating the type of regression model
SalePrice.Predict = predict(reg.nhood, boston.test),  # Predict 'SalePrice' using the fitted model
SalePrice.Error = SalePrice.Predict - SalePrice,  # Calculate the error between predicted and actual 'SalePrice'
SalePrice.AbsError = abs(SalePrice.Predict - SalePrice),  # Calculate the absolute error
SalePrice.APE = (abs(SalePrice.Predict - SalePrice)) / SalePrice) %>%  # Calculate the absolute percentage error
filter(SalePrice < 5000000)  # Filter out observations with 'SalePrice' greater than $5,000,000
bothRegressions <-
rbind(
dplyr::select(boston.test, starts_with("SalePrice"), Regression, Name) %>%
mutate(lagPriceError = lag.listw(spatialWeights.test, SalePrice.Error)),
dplyr::select(boston.test.nhood, starts_with("SalePrice"), Regression, Name) %>%
mutate(lagPriceError = lag.listw(spatialWeights.test, SalePrice.Error)))
bothRegressions <-
rbind(
dplyr::select(boston.test, starts_with("SalePrice"), Regression, Name) %>%
mutate(lagPriceError = lag.listw(spatialWeights.test, SalePrice.Error)),
dplyr::select(boston.test.nhood, starts_with("SalePrice"), Regression, Name) %>%
mutate(lagPriceError = lag.listw(spatialWeights.test, SalePrice.Error)))
st_drop_geometry(bothRegressions) %>%
gather(Variable, Value, -Regression, -Name) %>%
filter(Variable == "SalePrice.AbsError" | Variable == "SalePrice.APE") %>%
group_by(Regression, Variable) %>%
summarize(meanValue = mean(Value, na.rm = T)) %>%
spread(Variable, meanValue) %>%
kable()
bothRegressions %>%
dplyr::select(SalePrice.Predict, SalePrice, Regression) %>%
ggplot(aes(SalePrice, SalePrice.Predict)) +
geom_point() +
stat_smooth(aes(SalePrice, SalePrice),
method = "lm", se = FALSE, size = 1, colour="#FA7800") +
stat_smooth(aes(SalePrice.Predict, SalePrice),
method = "lm", se = FALSE, size = 1, colour="#25CB10") +
facet_wrap(~Regression) +
labs(title="Predicted sale price as a function of observed price",
subtitle="Orange line represents a perfect prediction; Green line represents prediction") +
theme_minimal()
st_drop_geometry(bothRegressions) %>%
group_by(Regression, Name) %>%
summarize(mean.MAPE = mean(SalePrice.APE, na.rm = T)) %>%
ungroup() %>%
left_join(nhoods, by = c("Name" = "neighborhood")) %>%
st_sf() %>%
ggplot() +
geom_sf(aes(fill = mean.MAPE)) +
geom_sf(data = bothRegressions, colour = "black", size = .5) +
facet_wrap(~Regression) +
scale_fill_gradient(brewer.pal(7, "Blues"),
name = "MAPE") +
labs(title = "Mean test set MAPE by neighborhood") +
theme_void()
tracts17 <-
get_acs(geography = "tract", variables = c("B01001_001E","B01001A_001E","B06011_001"),
year = 2017, state=25, county=025, geometry=T, output = "wide") %>%
st_transform('ESRI:102286')  %>%
rename(TotalPop = B01001_001E,
NumberWhites = B01001A_001E,
Median_Income = B06011_001E) %>%
mutate(percentWhite = NumberWhites / TotalPop,
raceContext = ifelse(percentWhite > .5, "Majority White", "Majority Non-White"),
incomeContext = ifelse(Median_Income > 32322, "High Income", "Low Income"))
grid.arrange(ncol = 2,
ggplot() + geom_sf(data = na.omit(tracts17), aes(fill = raceContext)) +
scale_fill_manual(values = c("#25CB10", "#FA7800"), name="Race Context") +
labs(title = "Race Context") +
theme_void()+ theme(legend.position="bottom"),
ggplot() + geom_sf(data = na.omit(tracts17), aes(fill = incomeContext)) +
scale_fill_manual(values = c("#25CB10", "#FA7800"), name="Income Context") +
labs(title = "Income Context") +
theme_void() + theme(legend.position="bottom"))
st_join(bothRegressions, tracts17) %>%
filter(!is.na(incomeContext)) %>%
group_by(Regression, incomeContext) %>%
summarize(mean.MAPE = scales::percent(mean(SalePrice.APE, na.rm = T))) %>%
st_drop_geometry() %>%
spread(incomeContext, mean.MAPE) %>%
kable(caption = "Test set MAPE by neighborhood income context")
